import time
import json
import os
import logging
import random
import schedule
import re
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager  # ì›¹ë“œë¼ì´ë²„ ìë™ ê´€ë¦¬

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("twitter_monitor.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ëª¨ë‹ˆí„°ë§í•  ì¸í”Œë£¨ì–¸ì„œ ëª©ë¡
influencers = [
    {"name": "Elon Musk", "twitter_username": "elonmusk", "coins": ["DOGE", "SHIB", "FLOKI"]},
    {"name": "Michael Saylor", "twitter_username": "saylor", "coins": ["BTC"]},
    {"name": "Vitalik Buterin", "twitter_username": "VitalikButerin", "coins": ["ETH"]},
    {"name": "Donald Trump", "twitter_username": "realDonaldTrump", "coins": ["TRUMP", "MAGA"]}
]

# ë¯¼ê° í‚¤ì›Œë“œ ì •ì˜
risk_keywords = {
    "HIGH": ["ETF", "SEC", "ê¸ˆë¦¬", "íŠ¸ëŸ¼í”„", "ë„ì§€ì½”ì¸", "DOGE", "SHIB", "ì œì¬", "íƒ„ì†Œ", "íŒŒì‚°", "ë¦¬ìŠ¤í¬", "íˆ¬ì ì¤‘ë‹¨"],
    "MEDIUM": ["ìƒìŠ¹", "í•˜ë½", "í˜¸ì¬", "ì•…ì¬", "í•©ì˜", "ì—…ë°ì´íŠ¸"],
    "LOW": []
}

# ì½”ì¸ë³„ íŒ¨í„´ ë°ì´í„° 
coin_patterns = {
    'ETH': {
        'avgReactionTimeMinutes': 12,
        'avgPriceImpactPercent': 8,
        'positiveKeywords': ['scaling', 'staking', 'defi', 'layer 2', 'upgrade', 'eth', 'ethereum'],
        'negativeKeywords': ['delay', 'issue', 'problem', 'bug']
    },
    'DOGE': {
        'avgReactionTimeMinutes': 7,
        'avgPriceImpactPercent': 12,
        'positiveKeywords': ['dog', 'moon', 'favorite', 'love', 'doge', 'dogecoin'],
        'negativeKeywords': ['sell', 'overvalued']
    },
    'BTC': {
        'avgReactionTimeMinutes': 10,
        'avgPriceImpactPercent': 5,
        'positiveKeywords': ['reserve', 'property', 'hope', 'acquire', 'hold', 'btc', 'bitcoin'],
        'negativeKeywords': ['sell', 'risk', 'ban', 'regulation']
    },
    'SHIB': {
        'avgReactionTimeMinutes': 8,
        'avgPriceImpactPercent': 15,
        'positiveKeywords': ['dog', 'community', 'cute', 'pet', 'shib', 'shiba'],
        'negativeKeywords': ['dump', 'meme', 'joke']
    },
    'FLOKI': {
        'avgReactionTimeMinutes': 5,
        'avgPriceImpactPercent': 25,
        'positiveKeywords': ['puppy', 'cute', 'moon', 'pet', 'floki'],
        'negativeKeywords': ['sell', 'scam', 'joke']
    },
    'TRUMP': {
        'avgReactionTimeMinutes': 15,
        'avgPriceImpactPercent': 35,
        'positiveKeywords': ['president', 'win', 'election', 'victory', 'trump'],
        'negativeKeywords': ['case', 'trial', 'verdict']
    },
    'MAGA': {
        'avgReactionTimeMinutes': 14,
        'avgPriceImpactPercent': 30,
        'positiveKeywords': ['america', 'win', 'great', 'huge', 'maga'],
        'negativeKeywords': ['lose', 'bad', 'fake']
    }
}

# ì´ë¯¸ ì²˜ë¦¬í•œ íŠ¸ìœ— ID ì €ì¥
processed_tweet_ids = set()

# ë¯¼ê°ë„ íŒë‹¨ í•¨ìˆ˜
def determine_risk(text):
    for level in ["HIGH", "MEDIUM"]:
        for word in risk_keywords[level]:
            if word.upper() in text.upper():
                return level
    return "LOW"

# ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (ì½”ì¸ë¦¬ë”ìŠ¤)
def fetch_coinreaders_news():
    url = "https://www.coinreaders.com/"
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        articles = soup.select("ul.list-type01 li a")

        news_events = []
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        for a in articles:
            title = a.get_text(strip=True)
            link = a.get("href")
            if not link.startswith("http"):
                link = "https://www.coinreaders.com" + link
            risk = determine_risk(title)
            if risk != "LOW":
                news_events.append({
                    "timestamp": now,
                    "source": "news",
                    "headline": title,
                    "risk_level": risk,
                    "url": link
                })
        return news_events
    except Exception as e:
        print(f"âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        return []

# ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
def initialize_webdriver():
    """webdriver-managerë¥¼ ì‚¬ìš©í•˜ì—¬ Chrome ì›¹ë“œë¼ì´ë²„ ìë™ ì´ˆê¸°í™”"""
    try:
        # Chrome ì˜µì…˜ ì„¤ì •
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (í™”ë©´ í‘œì‹œ X)
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-notifications")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--mute-audio")
        
        # ë¬´ì‘ìœ„ User-Agent ì„¤ì • (íƒì§€ ë°©ì§€)
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 11.5; rv:91.0) Gecko/20100101 Firefox/91.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59"
        ]
        chrome_options.add_argument(f"--user-agent={random.choice(user_agents)}")
        
        # ë¸Œë¼ìš°ì € ì°½ í¬ê¸° ì„¤ì •
        chrome_options.add_argument("--window-size=1920,1080")
        
        # ê¸°íƒ€ ìœ ìš©í•œ ì˜µì…˜
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option("useAutomationExtension", False)
        
        # ë¡œê¹… ë ˆë²¨ ì„¤ì • (ì›¹ë“œë¼ì´ë²„ ë§¤ë‹ˆì € ë¡œê·¸ ì¤„ì´ê¸°)
        import logging
        logging.getLogger('WDM').setLevel(logging.ERROR)
        
        # ìë™ìœ¼ë¡œ ìµœì‹  í¬ë¡¬ë“œë¼ì´ë²„ ì„¤ì¹˜ ë° ì„œë¹„ìŠ¤ ìƒì„±
        try:
            # í‘œì¤€ í¬ë¡¬ ë“œë¼ì´ë²„ ì‹œë„
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
        except Exception as chrome_error:
            logger.warning(f"ê¸°ë³¸ í¬ë¡¬ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨, ë‹¤ë¥¸ ë°©ë²• ì‹œë„: {chrome_error}")
            
            try:
                # ì§ì ‘ ëª…ì‹œì  ë²„ì „ ì§€ì • ì‹œë„
                try:
                    specific_version = "135.0.7049.0"  # í˜„ì¬ í¬ë¡¬ ë²„ì „ê³¼ í˜¸í™˜ë˜ëŠ” ë“œë¼ì´ë²„ ë²„ì „
                    service = Service(ChromeDriverManager(version=specific_version).install())
                    driver = webdriver.Chrome(service=service, options=chrome_options)
                except Exception as version_error:
                    logger.error(f"íŠ¹ì • ë²„ì „ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {version_error}")
                    raise
            except Exception as chrome_error2:
                logger.warning(f"ëª¨ë“  í¬ë¡¬ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨, Safari ì‹œë„: {chrome_error2}")
                return initialize_safari_webdriver()
        
        driver.set_page_load_timeout(30)  # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        
        logger.info("ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
        return driver
    
    except Exception as e:
        logger.error(f"ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        
        # Firefox ì›¹ë“œë¼ì´ë²„ë¡œ ëŒ€ì²´ ì‹œë„ (ì„ íƒì )
        try:
            from selenium.webdriver.firefox.options import Options as FirefoxOptions
            from webdriver_manager.firefox import GeckoDriverManager
            
            logger.info("Firefox ì›¹ë“œë¼ì´ë²„ë¡œ ëŒ€ì²´ ì‹œë„")
            
            firefox_options = FirefoxOptions()
            firefox_options.add_argument("--headless")
            
            driver = webdriver.Firefox(
                service=Service(GeckoDriverManager().install()),
                options=firefox_options
            )
            driver.set_page_load_timeout(30)
            
            logger.info("Firefox ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
            return driver
        except Exception as firefox_error:
            logger.error(f"Firefox ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì˜¤ë¥˜: {firefox_error}")
            return None

# Safari ë“œë¼ì´ë²„ ì´ˆê¸°í™” í•¨ìˆ˜ (macOS ì „ìš© ëŒ€ì•ˆ)
def initialize_safari_webdriver():
    """Safari ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” (macOS ì „ìš©)"""
    try:
        from selenium.webdriver.safari.options import Options as SafariOptions
        
        safari_options = SafariOptions()
        driver = webdriver.Safari(options=safari_options)
        driver.set_page_load_timeout(30)
        
        logger.info("Safari ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì„±ê³µ")
        return driver
    except Exception as e:
        logger.error(f"Safari ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return None

# íŠ¸ìœ—ì˜ ìµœì‹ ì„± í™•ì¸ í•¨ìˆ˜
def is_recent_tweet(created_at, max_days_old=2):
    """íŠ¸ìœ—ì´ ìµœê·¼ ê²ƒì¸ì§€ í™•ì¸ (ê¸°ë³¸ê°’: ìµœê·¼ 2ì¼ ì´ë‚´)"""
    if isinstance(created_at, str):
        try:
            created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
        except:
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ (ì•ˆì „)
            return True
    
    now = datetime.now()
    if not isinstance(created_at, datetime):
        return True  # í™•ì¸ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨
        
    # ì‹œê°„ëŒ€ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë¡œì»¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
    if created_at.tzinfo is not None:
        now = datetime.now(created_at.tzinfo)
        
    time_diff = now - created_at
    return time_diff.days <= max_days_old

# íŠ¸ìœ„í„°(X) ì ‘ì† ë° íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸°
def get_recent_tweets_via_selenium(username, max_tweets=5):
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ íŠ¸ìœ„í„°ì—ì„œ ìµœê·¼ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸°"""
    driver = None
    try:
        logger.info(f"{username}ì˜ ìµœê·¼ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸° ì‹œë„")
        
        # ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        driver = initialize_webdriver()
        if not driver:
            return []
        
        # íŠ¸ìœ— ìˆ˜ì§‘ ë°©ë²• 1: ì§ì ‘ íŠ¸ìœ„í„°(X) ì ‘ì†
        try:
            return get_tweets_from_twitter(driver, username, max_tweets)
        except Exception as twitter_error:
            logger.warning(f"íŠ¸ìœ„í„°ì—ì„œ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {twitter_error}")
            return []
    
    except Exception as e:
        logger.error(f"íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")
            except:
                pass

# íŠ¸ìœ„í„°ì—ì„œ ì§ì ‘ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸° (ë‚ ì§œ í˜•ì‹ ê¸°ë°˜ í•„í„°ë§ ê°œì„ )
def get_tweets_from_twitter(driver, username, max_tweets=5):
    """íŠ¸ìœ„í„°(X)ì—ì„œ ì§ì ‘ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸° - ìµœì‹  íŠ¸ìœ— í•„í„°ë§ ê°•í™”"""
    # íŠ¸ìœ„í„° í”„ë¡œí•„ í˜ì´ì§€ ì ‘ì†
    url = f"https://twitter.com/{username}"
    
    logger.info(f"íŠ¸ìœ„í„° URL ì ‘ì†: {url}")
    driver.get(url)
    
    # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, "//article[@data-testid='tweet']"))
        )
    except TimeoutException:
        logger.warning("íŠ¸ìœ„í„° í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (ë””ë²„ê¹…ìš©)
        try:
            os.makedirs('screenshots', exist_ok=True)
            screenshot_path = f"screenshots/twitter_{username}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            driver.save_screenshot(screenshot_path)
            logger.info(f"ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì˜¤ë¥˜: {e}")
            
        raise TimeoutException("íŠ¸ìœ„í„° í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
    
    # íŠ¸ìœ— ìš”ì†Œ ì°¾ê¸°
    tweet_elements = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
    
    if not tweet_elements:
        logger.warning(f"íŠ¸ìœ„í„°ì—ì„œ {username}ì˜ íŠ¸ìœ—ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        raise NoSuchElementException("íŠ¸ìœ— ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    logger.info(f"íŠ¸ìœ„í„°ì—ì„œ {len(tweet_elements)}ê°œì˜ íŠ¸ìœ— ìš”ì†Œ ë°œê²¬")
    
    # í•„ìš”í•œ ê²½ìš° ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ íŠ¸ìœ— ë¡œë“œ
    if len(tweet_elements) < max_tweets:
        scroll_twitter_page(driver, scroll_count=2)
        tweet_elements = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
        logger.info(f"ìŠ¤í¬ë¡¤ í›„ {len(tweet_elements)}ê°œì˜ íŠ¸ìœ— ìš”ì†Œ ë°œê²¬")
    
    # íŠ¸ìœ—ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    tweets = []
    recent_tweets = []  # ìµœì‹  íŠ¸ìœ—(ì—°ë„ í‘œì‹œ ì—†ëŠ”)
    older_tweets = []   # ì˜¤ë˜ëœ íŠ¸ìœ—(ì—°ë„ í‘œì‹œ ìˆëŠ”)
    
    for tweet_elem in tweet_elements:
        try:
            # íŠ¸ìœ— ID ì¶”ì¶œ
            links = tweet_elem.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
            if not links:
                continue
                
            href = links[0].get_attribute("href")
            tweet_id_match = re.search(r'/status/(\d+)', href)
            
            if not tweet_id_match:
                continue
                
            tweet_id = tweet_id_match.group(1)
            
            # ì´ë¯¸ ì²˜ë¦¬í•œ íŠ¸ìœ—ì¸ì§€ í™•ì¸
            if tweet_id in processed_tweet_ids:
                continue
            
            # íŠ¸ìœ— ë‚´ìš© ì¶”ì¶œ
            text_elements = tweet_elem.find_elements(By.XPATH, ".//div[@data-testid='tweetText']")
            if not text_elements:
                continue
                
            tweet_text = text_elements[0].text
            
            # ë¦¬íŠ¸ìœ— ì—¬ë¶€ í™•ì¸
            if "RT @" in tweet_text:
                continue
            
            # ë‚ ì§œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—°ë„ í¬í•¨ ì—¬ë¶€ í™•ì¸)
            date_text = ""
            is_recent = True  # ê¸°ë³¸ê°’ì€ ìµœì‹  íŠ¸ìœ—ìœ¼ë¡œ ê°€ì •
            
            try:
                # ì‹œê°„ ìš”ì†Œ ì°¾ê¸°
                time_elements = tweet_elem.find_elements(By.XPATH, ".//time")
                if time_elements:
                    # ë‚ ì§œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    date_text = time_elements[0].get_attribute("datetime")
                    
                    # í™”ë©´ì— í‘œì‹œë˜ëŠ” ë‚ ì§œ í…ìŠ¤íŠ¸ (ì—°ë„ í¬í•¨ ì—¬ë¶€ í™•ì¸ìš©)
                    displayed_date = ""
                    try:
                        displayed_date = time_elements[0].find_element(By.XPATH, "./..").text
                    except:
                        pass
                    
                    # ì—°ë„ê°€ í‘œì‹œë˜ì–´ ìˆìœ¼ë©´ ìµœì‹  íŠ¸ìœ—ì´ ì•„ë‹˜
                    # íŠ¸ìœ„í„°ëŠ” ìµœì‹  íŠ¸ìœ—ì— "në¶„ ì „", "nì‹œê°„ ì „", "nì¼ ì „" ë˜ëŠ” "1ì›” 15ì¼"ì²˜ëŸ¼ í‘œì‹œ (ì—°ë„ ì—†ìŒ)
                    # ì˜¤ë˜ëœ íŠ¸ìœ—ì€ "2023ë…„ 1ì›” 15ì¼"ì²˜ëŸ¼ ì—°ë„ë¥¼ í¬í•¨í•˜ì—¬ í‘œì‹œ
                    is_recent = "ë…„" not in displayed_date and "20" not in displayed_date[:4]
            except:
                pass
            
            # ë‚ ì§œ íŒŒì‹±
            created_at = datetime.now()  # ê¸°ë³¸ê°’
            try:
                if date_text:
                    created_at = datetime.fromisoformat(date_text.replace('Z', '+00:00'))
            except:
                pass
            
            # ì¢‹ì•„ìš” ìˆ˜, ë¦¬íŠ¸ìœ— ìˆ˜ ì¶”ì¶œ
            likes_count = 0
            retweets_count = 0
            
            try:
                metrics = tweet_elem.find_elements(By.XPATH, ".//*[@data-testid='like' or @data-testid='retweet']")
                for metric in metrics:
                    aria_label = metric.get_attribute("aria-label")
                    if not aria_label:
                        continue
                        
                    if "like" in aria_label.lower():
                        likes_text = aria_label.split()[0]
                        likes_count = parse_count(likes_text)
                    elif "retweet" in aria_label.lower():
                        retweets_text = aria_label.split()[0]
                        retweets_count = parse_count(retweets_text)
            except:
                pass
            
            # íŠ¸ìœ— ê°ì²´ ìƒì„±
            if is_recent:
                tweet = {
                    'id': tweet_id,
                    'text': tweet_text,
                    'created_at': created_at,
                    'author_id': username,
                    'public_metrics': {
                        'like_count': likes_count,
                        'retweet_count': retweets_count,
                        'reply_count': 0,
                        'quote_count': 0
                    },
                    'url': f"https://twitter.com/{username}/status/{tweet_id}",
                    'source': 'twitter',
                    'is_recent': is_recent
                }
                recent_tweets.append(tweet)
            
            # ID ê¸°ë¡ (ì¤‘ë³µ ë°©ì§€)
            processed_tweet_ids.add(tweet_id)
            
        except Exception as e:
            logger.error(f"íŠ¸ìœ„í„° íŠ¸ìœ— ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    # ìµœì‹  íŠ¸ìœ— ìš°ì„ , ì˜¤ë˜ëœ íŠ¸ìœ—ì€ ê·¸ ë‹¤ìŒì— ì¶”ê°€ (ìµœëŒ€ ê°œìˆ˜ ì œí•œ)
    tweets = recent_tweets + older_tweets
    if len(tweets) > max_tweets:
        tweets = tweets[:max_tweets]
    
    logger.info(f"íŠ¸ìœ„í„°ì—ì„œ {len(tweets)}ê°œì˜ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸° ì„±ê³µ (ìµœì‹  íŠ¸ìœ—: {len(recent_tweets)}ê°œ, ì˜¤ë˜ëœ íŠ¸ìœ—: {len(older_tweets)}ê°œ)")
    return tweets

# íŠ¸ìœ„í„° í˜ì´ì§€ ìŠ¤í¬ë¡¤ í•¨ìˆ˜ (ë” ë§ì€ íŠ¸ìœ— ë¡œë“œ)
def scroll_twitter_page(driver, scroll_count=3, wait_time=1):
    """íŠ¸ìœ„í„° í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ íŠ¸ìœ— ë¡œë“œ"""
    try:
        for i in range(scroll_count):
            # í˜ì´ì§€ ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ë¡œë”© ëŒ€ê¸°
            time.sleep(wait_time)
            
            # ìƒˆ íŠ¸ìœ— ë¡œë“œ í™•ì¸ ì‹œë„
            try:
                # ë” ë¡œë“œ ì¤‘ í‘œì‹œ í™•ì¸
                loading_elements = driver.find_elements(By.XPATH, "//div[contains(@aria-label, 'Loading')]")
                if loading_elements:
                    # ë¡œë”©ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°
                    time.sleep(wait_time * 2)
            except:
                pass
                
            logger.info(f"ìŠ¤í¬ë¡¤ {i+1}/{scroll_count} ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ìŠ¤í¬ë¡¤ ì˜¤ë¥˜: {e}")

# ìˆ«ì í…ìŠ¤íŠ¸ íŒŒì‹± (1.5K -> 1500)
def parse_count(count_text):
    try:
        count_text = str(count_text).strip()
        if 'K' in count_text or 'k' in count_text:
            return int(float(count_text.replace('K', '').replace('k', '')) * 1000)
        elif 'M' in count_text or 'm' in count_text:
            return int(float(count_text.replace('M', '').replace('m', '')) * 1000000)
        else:
            return int(count_text.replace(',', ''))
    except (ValueError, TypeError):
        return 0

# íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ íŠ¸ìœ— ê²€ìƒ‰
def search_tweets_for_keywords(tweets, keywords):
    """íŠ¸ìœ—ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œ ê²€ìƒ‰"""
    if not tweets or not keywords:
        return []
    
    matching_tweets = []
    
    for tweet in tweets:
        text = tweet['text'].lower()
        
        if any(keyword.lower() in text for keyword in keywords):
            matching_tweets.append(tweet)
    
    return matching_tweets

# íŠ¸ìœ—ì„ JSON íŒŒì¼ì— ì €ì¥
def save_tweets_to_file(tweets, username):
    """íŠ¸ìœ—ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if not tweets:
        return
        
    # í´ë” ìƒì„±
    os.makedirs('tweets', exist_ok=True)
    
    # íŒŒì¼ëª… ì„¤ì •
    filename = f"tweets/{username}_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(tweets, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"{username}ì˜ {len(tweets)}ê°œ íŠ¸ìœ—ì„ {filename}ì— ì €ì¥")
    except Exception as e:
        logger.error(f"íŠ¸ìœ— ì €ì¥ ì˜¤ë¥˜: {e}")

# ëª¨ë“  íŠ¸ìœ—ì„ í•œ íŒŒì¼ì— ì €ì¥ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
def update_all_tweets_file(username, tweets):
    """ëª¨ë“  íŠ¸ìœ—ì„ í•˜ë‚˜ì˜ íŒŒì¼ì— ì—…ë°ì´íŠ¸"""
    if not tweets:
        return
        
    # í´ë” ìƒì„±
    os.makedirs('tweets', exist_ok=True)
    
    # íŒŒì¼ëª… ì„¤ì •
    filename = 'tweets/all_tweets.json'
    
    try:
        # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
        all_tweets = {}
        
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                all_tweets = json.load(f)
        
        # ì‚¬ìš©ì íŠ¸ìœ— ì—…ë°ì´íŠ¸
        if username not in all_tweets:
            all_tweets[username] = []
            
        # ìƒˆ íŠ¸ìœ—ë§Œ ì¶”ê°€
        existing_ids = {t['id'] for t in all_tweets[username] if 'id' in t}
        new_tweets = [t for t in tweets if t['id'] not in existing_ids]
        
        if new_tweets:
            all_tweets[username] = new_tweets + all_tweets[username]
            
            # ìµœëŒ€ 100ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            all_tweets[username] = all_tweets[username][:100]
            
            # íŒŒì¼ì— ì €ì¥
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(all_tweets, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"{username}ì˜ {len(new_tweets)}ê°œ ìƒˆ íŠ¸ìœ—ì„ {filename}ì— ì¶”ê°€")
    except Exception as e:
        logger.error(f"íŠ¸ìœ— ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

# ì½”ì¸ë³„ íŠ¸ìœ— ë¶„ë¥˜ ë° ì €ì¥
def categorize_and_save_coin_tweets(all_new_tweets):
    """ì½”ì¸ë³„ë¡œ íŠ¸ìœ— ë¶„ë¥˜í•˜ì—¬ ì €ì¥"""
    # í´ë” ìƒì„±
    os.makedirs('tweets/coins', exist_ok=True)
    
    # ì½”ì¸ë³„ íŠ¸ìœ— í•„í„°ë§ ë° ì €ì¥
    for coin, pattern in coin_patterns.items():
        keywords = pattern.get('positiveKeywords', []) + pattern.get('negativeKeywords', []) + [coin.lower()]
        coin_tweets = []
        
        for username, tweets in all_new_tweets.items():
            matching_tweets = search_tweets_for_keywords(tweets, keywords)
            coin_tweets.extend(matching_tweets)
        
        if coin_tweets:
            # íŒŒì¼ëª… ì„¤ì •
            filename = f"tweets/coins/{coin}_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
            
            # íŒŒì¼ì— ì €ì¥
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(coin_tweets, f, ensure_ascii=False, indent=2, default=str)
                logger.info(f"{coin} ê´€ë ¨ {len(coin_tweets)}ê°œ íŠ¸ìœ—ì„ {filename}ì— ì €ì¥")
            except Exception as e:
                logger.error(f"{coin} íŠ¸ìœ— ì €ì¥ ì˜¤ë¥˜: {e}")


# íŠ¸ìœ— ëª¨ë‹ˆí„°ë§ ë° JSON ì¶œë ¥ í•¨ìˆ˜
def monitor_tweets():
    """ëª¨ë“  ì¸í”Œë£¨ì–¸ì„œì˜ íŠ¸ìœ—ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  JSONìœ¼ë¡œ ì €ì¥"""
    now = datetime.now().strftime("%H:%M:%S")
    logger.info(f"\n[{now}] === ì¸í”Œë£¨ì–¸ì„œ íŠ¸ìœ— ëª¨ë‹ˆí„°ë§ ===")
    
    all_new_tweets = {}
    
    # ëª¨ë“  ì¸í”Œë£¨ì–¸ì„œì— ëŒ€í•´ ìµœê·¼ íŠ¸ìœ— í™•ì¸
    for influencer in influencers:
        username = influencer["twitter_username"]
        
        logger.info(f"\nğŸ‘¤ {influencer['name']}ì˜ ìµœê·¼ íŠ¸ìœ— í™•ì¸ ì¤‘...")
        
        # Seleniumì„ ì‚¬ìš©í•˜ì—¬ ìµœê·¼ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸°
        tweets = get_recent_tweets_via_selenium(username)
        
        if not tweets:
            logger.warning(f"âš ï¸ {username}ì˜ íŠ¸ìœ—ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue
            
        logger.info(f"âœ… {len(tweets)}ê°œì˜ íŠ¸ìœ—ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        
        # ê° íŠ¸ìœ— ì¶œë ¥
        for tweet in tweets:
            created_at = tweet['created_at'].strftime("%Y-%m-%d %H:%M") if hasattr(tweet['created_at'], 'strftime') else tweet['created_at']
            logger.info(f"- [{created_at}] {tweet['text'][:100]}{'...' if len(tweet['text']) > 100 else ''}")
            
            # ì½”ì¸ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì‚¬ ë° ì•Œë¦¼
            for coin in influencer["coins"]:
                keywords = coin_patterns.get(coin, {}).get('positiveKeywords', []) + coin_patterns.get(coin, {}).get('negativeKeywords', []) + [coin.lower()]
                if any(keyword.lower() in tweet['text'].lower() for keyword in keywords):
                    logger.info(f"ğŸš¨ {coin} ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€: {tweet['url'] or ''}")
        
        # íŠ¸ìœ— ì €ì¥
        save_tweets_to_file(tweets, username)
        update_all_tweets_file(username, tweets)
        
        # ìƒˆ íŠ¸ìœ— ê¸°ë¡
        all_new_tweets[username] = tweets
    
    # ì½”ì¸ë³„ íŠ¸ìœ— ë¶„ë¥˜ ë° ì €ì¥
    categorize_and_save_coin_tweets(all_new_tweets)
    
    return all_new_tweets

# ë©”ì¸ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("===== Selenium ê¸°ë°˜ ì‹¤ì‹œê°„ íŠ¸ìœ„í„° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘ =====")
        
        # ì²« ì‹¤í–‰
        monitor_tweets()
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì • (1ë¶„ë§ˆë‹¤)
        schedule.every(1).minutes.do(monitor_tweets)
        
        # ë©”ì¸ ë£¨í”„
        logger.info("ìŠ¤ì¼€ì¤„ë§ ì‹œì‘... (Ctrl+Cë¡œ ì¤‘ì§€)")
        while True:
            schedule.run_pending()
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("\ní”„ë¡œê·¸ë¨ ì¢…ë£Œ...")
    except Exception as e:
        logger.error(f"\nâš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())

# í”„ë¡œê·¸ë¨ ì‹œì‘
if __name__ == "__main__":
    main()